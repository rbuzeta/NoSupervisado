{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iE9XTGLU2V"
      },
      "source": [
        "# T2.3 - Métodos de clustering espectral\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpRUPzfm-ZM8"
      },
      "source": [
        "En esta práctica vamos a ver cómo funciona el clustering espectral.\n",
        "\n",
        "Para ello, como siempre, lo primero es cargar las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPsbbl_TR6cw"
      },
      "source": [
        "import numpy as np\n",
        "float_formatter = lambda x: \"%.3f\" % x\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from matplotlib import pyplot as plt\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ArN8e4RIrN"
      },
      "source": [
        "En nuestro primer ejemplo vamos a usar un dataset muy sencillo que nos permita entender lo que hacemos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JczwWrBNRIYY"
      },
      "source": [
        "X = np.array([\n",
        "    [1, 3], [2, 1], [1, 1],\n",
        "    [3, 2], [7, 8], [9, 8],\n",
        "    [9, 9], [8, 7], [13, 14],\n",
        "    [14, 14], [15, 16], [14, 15]\n",
        "])\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1])\n",
        "for i, txt in enumerate(range(X.shape[0])):\n",
        "    ax.annotate(txt, X[i])\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Largo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8YD9ONVSWrG"
      },
      "source": [
        "Tal y como acabamos de ver, el agrupamiento espectral consiste en 4 pasos básicos:\n",
        "\n",
        "-   la obtención de la matriz de adyacencias o afinidad,\n",
        "-   la obtención de la matriz Laplaciana,\n",
        "-   el cálculo de los vectores y valores propios de esta última,\n",
        "-   y el clustering mediante K-means (u otra técnica tradicional).\n",
        "\n",
        "Vamos a por la primera, la creación de la matriz de adyacencias o afinidad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A_u19doRIVj"
      },
      "source": [
        "W_dist =## Aquí tu código ##\n",
        "print(W_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "A1qLNOozo7wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-RmbITJ29hQ"
      },
      "source": [
        "W_dist.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXKOs8MQbeZc"
      },
      "source": [
        "plt.imshow(W_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeBtY_wRTHB2"
      },
      "source": [
        "# construímos la matriz de adyacencias o afinidad\n",
        "W_ad = np.zeros_like(W_dist, np.uint8)\n",
        "W_ad[W_dist < 5 ] = 1\n",
        "print(W_ad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai-lFHmHTpeh"
      },
      "source": [
        "Vamos a utilizar el paquete networkx para visualizar el grafo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Aeg2hJpRIQv"
      },
      "source": [
        "def draw_graph(G):\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw_networkx_nodes(G, pos)\n",
        "    nx.draw_networkx_labels(G, pos)\n",
        "    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf7Ya4W6RIN4"
      },
      "source": [
        "G = nx.DiGraph(np.array(W_ad))\n",
        "draw_graph(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "137TFJ-BUnPO"
      },
      "source": [
        "`networkx` también nos permite calcular la matriz de afinidad a partir del grafo. Vamos a ver como:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HosUsXysRILI"
      },
      "source": [
        "W_ad = nx.adjacency_matrix(G)\n",
        "print(W_ad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1AN_wZVOiq"
      },
      "source": [
        "Fijaos que esto que nos devuelve no parece una matriz, verdad? Habéis oído hablar del término *sparse*? Pues esto que acabamos de ver es una matriz *sparse*, que simplemente es una matriz con muy pocos elementos diferentes de 0.\n",
        "\n",
        "Para verla en formato matriz necesitamos hacer uso del método `todense()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptg_qOY3RIIE"
      },
      "source": [
        "print(W_ad.todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yprms7ohVqXQ"
      },
      "source": [
        "Ahora tenemos que obtener la matriz Laplaciana, que recordad que se obtenía como la resta entre la matriz de adyacencia y la matriz de grado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFVfUERRIDa"
      },
      "source": [
        "# matriz de grado\n",
        "D = ## Aquí tu código ##\n",
        "print('Matriz de grado:\\n', D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwztnBdORH5h"
      },
      "source": [
        "# matriz laplaciana\n",
        "L = ## Aquí tu código ##\n",
        "print('Matriz Laplaciana:\\n', L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OL793P0WH1b"
      },
      "source": [
        "La matriz Laplaciana es una matriz especial con ciertas propiedades que nos facilitan la vida. Una de ellas es que:\n",
        "\n",
        "-   **si el grafo `W` tiene `K` componentes conexas, entonces `L` tiene `K` vectores propios (*eigenvectors*) con valor propio (*eigenvalue*) igual a 0.**\n",
        "\n",
        "En este caso, cuantos eigenvectors vamos a tener iguales a 0?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--qzZ4eXWHru"
      },
      "source": [
        "e, v = ## Aquí tu código ##\n",
        "# eigenvalues\n",
        "print('eigenvalues:')\n",
        "print(e)\n",
        "# eigenvectors\n",
        "print('eigenvectors:')\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RQtg17hXWWE"
      },
      "source": [
        "En efecto, 3! No nos debería sorprender, ya que nuestro conjunto de datos tiene 3 clusters conexos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmWEZ2eeWHoS"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(e)\n",
        "plt.title('eigenvalues')\n",
        "\n",
        "eigen_0 = np.where(e < 10e-6)[0]\n",
        "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
        "for n, i in enumerate(eigen_0):\n",
        "    plt.figure()\n",
        "    plt.plot(v[:, i])\n",
        "    plt.title(f'{n} eigenvector con eigenvalue=0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du5LkK6xWHl4"
      },
      "source": [
        "# veamos nuestro dataset de nuevo\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')\n",
        "for i, txt in enumerate(range(X.shape[0])):\n",
        "    ax.annotate(txt, X[i])\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Largo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkqy5bRZv56"
      },
      "source": [
        "Si nos fijamos en las gráficas de los 3 primeros vectores propios de la matriz L (cuyos valores propios son 0), podemos ver cómo nos permiten separar correctamente los 3 clusters de nuestro dataset trazando una línea horizontal.\n",
        "\n",
        "Recordad que esto es buena señal, nos indica que con esta nueva representación de los datos, un algoritmo de clustering podrá separarlos correctamente.\n",
        "\n",
        "Vamos a comprobar si es cierto:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eigen_0"
      ],
      "metadata": {
        "id": "Ttw18fScfl_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpNusooYWHjQ"
      },
      "source": [
        "vectores_propios_a_utilizar = eigen_0 # en este caso queremos usar los 3, podría ser diferente\n",
        "\n",
        "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
        "print(Xnew.shape)\n",
        "km = KMeans(init='k-means++', n_clusters=len(eigen_0))\n",
        "km.fit(Xnew)\n",
        "km.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--tevhxAWHgZ"
      },
      "source": [
        "# veamos el resultado del clustering\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], c=km.labels_)\n",
        "for i, txt in enumerate(range(X.shape[0])):\n",
        "    ax.annotate(txt, X[i])\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Largo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgghAcvbdZP"
      },
      "source": [
        "¿Y si visualizamos los datos proyectados usando los 3 vectores propios?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69g9goaJWHd1"
      },
      "source": [
        "# veamos el dataset proyectado usando los 3 vectores propios escogidos\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Xnew[:, 0], Xnew[:, 1], c=km.labels_)\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Largo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZqn_-Sb1qC"
      },
      "source": [
        "Fijaos lo fácil que debe ser ahora para el k-means con esta nueva representación de los datos.\n",
        "\n",
        "Como habréis podido comprobar, al final, lo único que estamos haciendo es cambiar el espacio de representación de los datos por uno que nos permita agruparlos mejor. ¿Os suena esto de algo?\n",
        "\n",
        "En efecto, es básicamente el modo de funcionamiento de las redes neuronales profundas, en las que los datos van sufriendo transformaciones en cada capa hasta llegar a una representación que facilita la tarea a desarrollar.\n",
        "\n",
        "\"Pero, este ejemplo es muy sencillo\", podéis estar pensando. Y no os falta razón. Vamos a complicarlo un poco:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SAseW2wWHar"
      },
      "source": [
        "X, clusters = make_circles(n_samples=1000, noise=.05, factor=.5, random_state=0)\n",
        "print(X.shape)\n",
        "plt.scatter(X[:,0], X[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0N9tu6cf53"
      },
      "source": [
        "Vamos a probar con el k-means, a ver qué tal..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4onVZLqWHQn"
      },
      "source": [
        "km = ## Aquí tu código ##\n",
        "km_clustering = km.fit(X)\n",
        "plt.scatter(X[:,0], X[:,1], c=km_clustering.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGcwcPihctWE"
      },
      "source": [
        "Parece que no os he engañado, el k-means no es capaz de hacer el agrupamiento de forma correcta.\n",
        "\n",
        "Vamos a ver si con lo que acabamos de aprender somos capaces. Para ello, recordad que necesitamos:\n",
        "\n",
        "-   la obtención de la matriz de adyacencias o afinidad,\n",
        "-   la obtención de la matriz Laplaciana,\n",
        "-   el cálculo de los vectores y valores propios de esta última,\n",
        "-   transformación de los datos en el nuevo espacio\n",
        "-   y el clustering mediante K-means (u otra técnica tradicional).\n",
        "\n",
        "Manos a la obra:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXS0LMnTddgz"
      },
      "source": [
        "W_dist = ## Aquí tu código ##\n",
        "print('Matriz distancias:\\n', W_dist)\n",
        "plt.imshow(W_dist)\n",
        "plt.colorbar()\n",
        "plt.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUeTehYddeM"
      },
      "source": [
        "# Vamos a escoger 0.5 como el umbral\n",
        "W_ad = np.zeros_like(W_dist, np.uint8)\n",
        "W_ad[## Aquí tu código ##] = 1\n",
        "print('Matriz de adyacencia:\\n', W_ad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1LQsPqfddbz"
      },
      "source": [
        "# matriz de grado\n",
        "D = ## Aquí tu código ##\n",
        "print('Matriz de grado:\\n', D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH3ZACRBddZD"
      },
      "source": [
        "# matriz laplaciana\n",
        "L = ## Aquí tu código ##\n",
        "print('Matriz Laplaciana:\\n', L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLIXJLBrddWj"
      },
      "source": [
        "# calculamos los eigenvectors y eigenvalues\n",
        "e, v = ## Aquí tu código ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GWH67f5ddT0"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(e)\n",
        "plt.title('eigenvalues')\n",
        "\n",
        "eigen_0 = np.isclose(e, 0, atol=1e-3)\n",
        "print(f'Tenemos {sum(eigen_0)} componentes conexos en nuestro dataset.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9cOJS7iZlA"
      },
      "source": [
        "Fijaos que mirando los eigenvalues no parece que sea fácil elegir con qué componentes queremos quedarnos para hacer el clustering, ¿verdad?\n",
        "\n",
        "¿Se os ocurre alguna forma mejor de seguir?\n",
        "\n",
        "¿Probamos a calcular la matriz laplaciana normalizada simétrica a ver si mejora?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZVv6l0fghTt"
      },
      "source": [
        "# calculamos la laplaciana normalizada simétrica\n",
        "D = np.sum(W_ad, axis=1)\n",
        "D = D**(-1./2)\n",
        "I = np.diag(np.ones(D.size))\n",
        "D = np.diag(D)\n",
        "L = I-D.dot(W_ad).dot(D)\n",
        "\n",
        "# calculamos autovectores y autovalores\n",
        "e, v = ## Aquí tu código ##\n",
        "e = e.real\n",
        "v = v.real\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(e)\n",
        "plt.title('eigenvalues')\n",
        "\n",
        "eigen_0 = np.where(np.isclose(e, 0, atol=1e-1))[0]\n",
        "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
        "for n, i in enumerate(eigen_0):\n",
        "    plt.figure()\n",
        "    plt.plot(v[:, i])\n",
        "    plt.title(f'{n} eigenvector con eigenvalue=0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdER_bliumq"
      },
      "source": [
        "En este caso parece que encuentra las 3 componentes conexas! Vamos a ver si funciona el clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GamcL2_WhE0y"
      },
      "source": [
        "vectores_propios_a_utilizar = eigen_0\n",
        "print(vectores_propios_a_utilizar)\n",
        "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
        "km = KMeans(init='k-means++', n_clusters=2)\n",
        "km.fit(Xnew)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTi9MgX4h_5S"
      },
      "source": [
        "# veamos el resultado del clustering\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], c=km.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axQTgWsPi64o"
      },
      "source": [
        "Parece que no. Vamos a ver los datos en su nuevo espacio de representación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sOiT5N6i4s3"
      },
      "source": [
        "# veamos el dataset proyectado usando los 3 vectores propios escogidos\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Xnew[:,0], Xnew[:,1], c=km.labels_, alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRkCjlYkgDhi"
      },
      "source": [
        "Fijaos que mirando la nueva representación de los datos, no parece que el k-means vaya a poder hacer bien el clustering, ¿verdad?\n",
        "\n",
        "¿Se os ocurre alguna forma mejor de seguir?\n",
        "\n",
        "¿Quizá pensando en cómo hemos construído la matriz de adyacencia? ¿Existe alguna otra forma de hacerlo?\n",
        "\n",
        "Eso es, podemos intentarlo haciendo uso del kNN en vez del umbral ($\\epsilon$).\n",
        "\n",
        "Vamos allá:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h-Ghy01ddQz"
      },
      "source": [
        "def matriz_afinidad_KNN(mSimilitud, KNN=5):\n",
        "    auxM = mSimilitud.copy()\n",
        "    np.fill_diagonal(auxM, 0)\n",
        "\n",
        "    # Construimos la matriz afinidad de A a B\n",
        "    mAfinidadA = np.zeros(auxM.shape)\n",
        "    # utilizamos la matriz similitud para ordenar los vecinos de cada nodo por cercanía\n",
        "    # `np.argsort` nos devuelve los índices que ordenan el array introducido (de forma ascendente)\n",
        "    # en este caso, al tener la matriz similitud, tenemos que hacerla negativa, para que nos\n",
        "    # devuelva primero los más similares (que al ponerle el - a la matriz de similitud, son los\n",
        "    # valores más pequeños)\n",
        "    # Flatten simplemente lo utilizamos para convertir la matriz `KNN x n`en un vector de 1x(KNN x n)\n",
        "    indices_kNN_nodo_fila = np.argsort(-auxM, axis=0)[0:KNN, :].flatten()\n",
        "    # Nos creamos un array para combinar con el anterior y poder indexar los\n",
        "    # elementos pertinentes de la matriz de afinidad\n",
        "    indices_kNN_nodo_col = np.tile(np.arange(auxM.shape[0]), KNN)\n",
        "    mAfinidadA[indices_kNN_nodo_fila, indices_kNN_nodo_col] = 1\n",
        "    np.fill_diagonal(mAfinidadA, 1)\n",
        "\n",
        "    # Y ahora hacemos lo mismo de B a A\n",
        "    mAfinidadB = np.zeros(auxM.shape)\n",
        "    # Fijaos que en este caso la matriz va a ser `n x KNN` (hacemos el argsort en la dirección columnas: -->)\n",
        "    indices_kNN_nodo_fila = np.repeat(np.arange(auxM.shape[0]), KNN)\n",
        "    indices_kNN_nodo_col = np.argsort(-auxM, axis=1)[:, 0:KNN].flatten()\n",
        "    mAfinidadB[indices_kNN_nodo_fila, indices_kNN_nodo_col] = 1\n",
        "    np.fill_diagonal(mAfinidadB, 1)\n",
        "\n",
        "    return (mAfinidadA + mAfinidadB) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35bSTa4VNXx2"
      },
      "source": [
        "# calculamos de nuevo la matriz de adyacencia (o afinidad), esta vez siguiendo el enfoque de los kNN\n",
        "sigma = 0.1\n",
        "W_sim = np.exp(-np.power(W_dist,2)/(2*sigma**2))\n",
        "W_ad = matriz_afinidad_KNN(W_sim)\n",
        "print(W_ad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRDBlsZYddOM"
      },
      "source": [
        "# calculamos la laplaciana normalizada simétrica\n",
        "D = np.sum(W_ad,axis=1)\n",
        "D = D**(-1./2)\n",
        "I = np.diag(np.ones(D.size))\n",
        "D = np.diag(D)\n",
        "L = I - D.dot(W_ad).dot(D)\n",
        "\n",
        "# calculamos autovectores y autovalores\n",
        "e, v = np.linalg.eig(L)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(e)\n",
        "plt.title('eigenvalues')\n",
        "\n",
        "eigen_0 = np.where(np.isclose(e, 0, atol=1e-5))[0]\n",
        "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
        "for n, i in enumerate(eigen_0):\n",
        "    plt.figure()\n",
        "    plt.plot(v[:, i])\n",
        "    plt.title(f'{n} eigenvector con eigenvalue=0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3hEMqIXkNbr"
      },
      "source": [
        "Parece que ahora encuentra 2 componentes conexos! Vamos a probar a hacer el clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnR5__w2ddLi"
      },
      "source": [
        "vectores_propios_a_utilizar = eigen_0\n",
        "print(vectores_propios_a_utilizar)\n",
        "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
        "km = ## Aquí tu código ##\n",
        "km.fit(Xnew)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxWozte5dYVx"
      },
      "source": [
        "Veamos el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FlHni66ki5l"
      },
      "source": [
        "# veamos el resultado del clustering\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1], c=km.labels_, cmap='jet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQVga8HdHDn"
      },
      "source": [
        "Fijaos como es muy importante elegir el tipo de enfoque adecuado dependiendo de los datos. Cuando hemos construido la $W_{ad}$ mediante el método del umbral, nos hemos inventado completamente dicho umbral.\n",
        "\n",
        "Esto no quiere decir que no exista un umbral para el cual no funcione correctamente el clustering siguiendo ese enfoque, lo que quiere decir es que si no conocemos el umbral, es posiblemente más sencillo seguir el enfoque de los kNN.\n",
        "\n",
        "Vamos a ver los datos en su nuevo espacio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3oM8GERlDbb"
      },
      "source": [
        "# veamos el dataset proyectado usando los 2 vectores propios escogidos\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Xnew[:,0], Xnew[:,1], c=km.labels_, alpha=0.7, cmap='jet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo3qomiulFm_"
      },
      "source": [
        "Maravilloso, ¿no os parece?\n",
        "\n",
        "\n",
        "Y más maravilloso os va a parecer aún lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMZTNn5_cs95"
      },
      "source": [
        "sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=0)\n",
        "sc_clustering = sc.fit(X)\n",
        "plt.scatter(X[:,0], X[:,1], c=sc_clustering.labels_, cmap='jet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beWrCFQcspj"
      },
      "source": [
        "En efecto, así de rápido podemos realizar un clustering espectral utilizando las librerías existentes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BErmLZr8ghjU"
      },
      "source": [
        "Fuente: https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045\n",
        "\n",
        "Ejemplo paso a paso para entender y profundizar:\n",
        "-  https://towardsdatascience.com/spectral-clustering-aba2640c0d5b\n",
        "\n",
        "Más ejemplos:\n",
        "\n",
        "-  https://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html\n",
        "-  https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\n",
        "\n",
        "Lecturas discutiendo la similitud de k-means, clustering espectral y PCA, para quien tenga interés:\n",
        "\n",
        "-  https://stats.stackexchange.com/a/151665\n",
        "-  https://stats.stackexchange.com/a/189324\n",
        "-  https://stats.stackexchange.com/a/189324 (muy completa y didáctica)\n",
        "-  https://qr.ae/TW25h8\n"
      ]
    }
  ]
}