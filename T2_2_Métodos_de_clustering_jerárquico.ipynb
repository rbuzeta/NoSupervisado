{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iE9XTGLU2V"
      },
      "source": [
        "# T2.2 - Métodos de clustering jerárquico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpRUPzfm-ZM8"
      },
      "source": [
        "En esta práctica vamos a ver cómo funcionan los algoritmos jerárquicos vistos en clase: el aglomerativo y el divisivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-2-Apoz-4s_"
      },
      "source": [
        "### Clustering jerárquico aglomerativo\n",
        "\n",
        "En esta práctica estudiaremos el funcionamiento y la utilización del clústering jerárquico aglomerativo.\n",
        "\n",
        "Para empezar, cargamos las librerías que vamos a necesitar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz4xOnkeECZV"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage #nos permiten hacer un clustering jerárquico aglomerativo\n",
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV79MxIuERmE"
      },
      "source": [
        "### Ejemplo 1\n",
        "\n",
        "Generamos nuestro dataset de juguete:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKvIIhsET-t"
      },
      "source": [
        "# generate two clusters: a with 100 points, b with 50:\n",
        "np.random.seed(4711)  # for repeatability of this tutorial\n",
        "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
        "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
        "X = np.concatenate((a, b),)\n",
        "print(X.shape)  # 150 samples with 2 dimensions\n",
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrT7c9MzEb_f"
      },
      "source": [
        "Ya tenemos nuestro dataset, que como podéis comprobar es bien sencillo, para poder comprender bien el funcionamiento del clustering jerárquico.\n",
        "\n",
        "Vamos a realizar el clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszDl0OhEUit"
      },
      "source": [
        "Z = ## Aquí tu código ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-7_xtjuE1C9"
      },
      "source": [
        "Ya está. Fácil, ¿no os parece?\n",
        "\n",
        "Fijaos que `scipy` nos permite elegir qué método queremos elegir entre:\n",
        "\n",
        "-   `single`: disimilitud mínima\n",
        "-   `complete`: disimilitud máxima\n",
        "-   `average`: disimilitud media\n",
        "-   ... y más\n",
        "\n",
        "Y también nos permite elegir la medida de disimilitud que queremos utilizar para calcular las distancias entre clusters: `euclidean` (por defecto), `cityblock` (Manhattan), `hamming`, `cosine`, etc.\n",
        "\n",
        "Recordad que se debe escoger la medida de disimilitud teniendo en cuenta el tipo de datos que tenemos. Por ejemplo, si tuviesemos vectores de características binarios, usaríamos la distancia `hamming`.\n",
        "\n",
        "Vamos a medir la bondad de este clustering. Para ello, vamos a hacer uso del `Cophenetic Correlation Coefficient` (https://en.wikipedia.org/wiki/Cophenetic_correlation), disponible en la función `cophenet()` de `scipy.cluster.hierarchy`.\n",
        "\n",
        "Esta función compara (correla) las distancias reales entre todas las muestras de nuestro dataset con aquellas que implica el clustering obtenido. Cuanto más cerca este el valor de 1, mejor preserva el clustering las distancias originales, por lo que mejor es el clusering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2kMMf4bEz5R"
      },
      "source": [
        "from scipy.cluster.hierarchy import cophenet\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# pdist calcula las distancias entre elementos de nuestro dataset usando la\n",
        "# métrica indicada (euclideana por defecto)\n",
        "# Z tiene las distancias que se van uniendo cada vez\n",
        "c, coph_dists = cophenet(## Aquí tu código ##)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XQG1NjoG1b4"
      },
      "source": [
        "No importa qué método o métrica elijamos, la función `linkage` utilizará ese método y métrica para calcular las distancias entre los clusters (empezando por `k=n_instancias` clusters que tendrá al principio, es decir, cada muestra del dataset es un cluster) y en cada iteración mezclará los dos clusters con la distancia más pequeña de acuerdo al método y distancia elegidos.\n",
        "\n",
        "La salida de esta función (`Z`) es un vector de longitud `k-1` que nos dará información de las operaciones realizadas en cada paso. Es decir, las `k-1` uniones que han tenido lugar durante el clustering.\n",
        "\n",
        "`Z[i]` nos dirá qué clusters fueron unidos en la iteración `i`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq-oR6D3EupX"
      },
      "source": [
        "Z[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9kOm-uvHzXg"
      },
      "source": [
        "La salida es la siguiente:\n",
        "\n",
        "`[idx1, idx2, distancia, número de muestras]`\n",
        "\n",
        "En este caso podéis ver cómo el algoritmo ha decidido unir los clusters 52 y 53, que tenían una distancia de 0.04151, y esta operación dió como resultado un cluster con 2 instancias. `idx1` e `idx2` se corresponden con los índices de nuestras muestras. Recordad que nuestro dataset tiene 150 muestras.\n",
        "\n",
        "Vamos a ver las 20 primeras operaciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ7-9QxdHxRF"
      },
      "source": [
        "Z[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeVyAfd0Irv_"
      },
      "source": [
        "Fijaos en lós índices, ¿véis algo raro? Fijaos también en la columna de número de muestras. ¿Qué esta pasando?\n",
        "\n",
        "Lo que esta pasando es que cada vez que ocurre una unión, se crea un nuevo cluster con un nuevo índice (por eso hay más de 150, que es el número de muestras de nuestro dataset == el número de clusters iniciales).\n",
        "\n",
        "De hecho, todos los clusters cuyo índice es mayor que 149 (`idx >= len(X)`), realmente se refieren al cluster formado en la unión `Z[idx - len(X)`.\n",
        "\n",
        "Así, el `idx` 149 se corresponde con `X[149]`, pero el `idx=150` se corresponde con el cluster formado en el paso `Z[0]`, y así sucesivamente.\n",
        "\n",
        "Si os fijáis en la iteración 13, se mezclan el cluster 62 con el 152. ¿Y qué es lo que hay en el cluster 152? Pues `Z[152-150]=Z[2]`, por lo tanto, lo que se unió en la iteración 3 (`Z` está indexado en 0). Es decir, el cluster 33 y 68.\n",
        "\n",
        "Vamos a ver las coordenadas para ver si tiene sentido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVJyE0mIO0t"
      },
      "source": [
        "idxs = [33, 68, 62]\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X[:,0], X[:,1])  # plot all points\n",
        "plt.scatter(X[idxs,0], X[idxs,1], c='r')  # plot interesting points in red again\n",
        "plt.show()\n",
        "\n",
        "print(X[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzjW4g3DMcAg"
      },
      "source": [
        "Fijaos que los puntos realmente están bastante cerca los unos de los otros, así que tiene sentido.\n",
        "\n",
        "Vamos ahora a ver el dendograma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjg13qmxL0fm"
      },
      "source": [
        "# calculate full dendrogram\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Dendrograma')\n",
        "plt.xlabel('índice de la muestra')\n",
        "plt.ylabel('distancia')\n",
        "dendrogram(\n",
        "    Z,\n",
        "    leaf_rotation=90.,  # rotates the x axis labels\n",
        "    leaf_font_size=8.,  # font size for the x axis labels\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6gRgW1jM-hZ"
      },
      "source": [
        "Resumiendo el dendograma:\n",
        "\n",
        " -   Las lineas horizontales son uniones de clusters\n",
        " -   Las líneas verticales nos indican que clústers son parte de cada unión\n",
        " -   Las alturas de las líneas verticales nos indican la distancia que se \"cubrió\" al hacer la unión\n",
        "\n",
        " Además, el dendograma nos permite hacer cosas bastante útiles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-TwgafXMr3C"
      },
      "source": [
        "# quedarnos con las últimas p uniones\n",
        "plt.title('Hierarchical Clustering Dendrograma (truncado)')\n",
        "plt.xlabel('índice de la muestra')\n",
        "plt.ylabel('distancia')\n",
        "dendrogram(\n",
        "    Z,\n",
        "    truncate_mode='lastp',  # mostrar solo las últimas p uniones\n",
        "    p=12,  # definimos p\n",
        "    leaf_rotation=90.,\n",
        "    leaf_font_size=12.,\n",
        "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dVpNa7iO7EC"
      },
      "source": [
        "def fancy_dendrogram(*args, **kwargs):\n",
        "    max_d = kwargs.pop('max_d', None)\n",
        "    if max_d and 'color_threshold' not in kwargs:\n",
        "        kwargs['color_threshold'] = max_d\n",
        "    annotate_above = kwargs.pop('annotate_above', 0)\n",
        "\n",
        "    ddata = dendrogram(*args, **kwargs)\n",
        "\n",
        "    if not kwargs.get('no_plot', False):\n",
        "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
        "        plt.xlabel('sample index or (cluster size)')\n",
        "        plt.ylabel('distance')\n",
        "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
        "            x = 0.5 * sum(i[1:3])\n",
        "            y = d[1]\n",
        "            if y > annotate_above:\n",
        "                plt.plot(x, y, 'o', c=c)\n",
        "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
        "                             textcoords='offset points',\n",
        "                             va='top', ha='center')\n",
        "        if max_d:\n",
        "            plt.axhline(y=max_d, c='k')\n",
        "    return ddata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfgmZoE2O9Pl"
      },
      "source": [
        "fancy_dendrogram(\n",
        "    Z,\n",
        "    truncate_mode='lastp',\n",
        "    p=12,\n",
        "    leaf_rotation=90.,\n",
        "    leaf_font_size=12.,\n",
        "    show_contracted=True,\n",
        "    annotate_above=1,  # useful in small plots so annotations don't overlap\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW01TT_FOiIS"
      },
      "source": [
        "Vamos ahora a seleccionar en qué punto del proceso nos quedamos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Vxg9S3OlW-"
      },
      "source": [
        "d_max = 4\n",
        "\n",
        "\n",
        "fancy_dendrogram(\n",
        "    Z,\n",
        "    truncate_mode='lastp',\n",
        "    p=12,\n",
        "    leaf_rotation=90.,\n",
        "    leaf_font_size=12.,\n",
        "    show_contracted=True,\n",
        "    annotate_above=1,\n",
        "    max_d=d_max,  # plot a horizontal cut-off line\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWmUkeAjPTy2"
      },
      "source": [
        "Fuente: https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9-eeuAePUk5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqHw7eXePfnq"
      },
      "source": [
        "### Ejemplo 2\n",
        "\n",
        "Vamos a ver un segundo ejemplo, esta vez real.\n",
        "\n",
        "Imaginaos que pertenecéis a la junta directiva de un centro comercial y queréis hacer un estudio de qué perfil de cliente es el más rentable para vosotros.\n",
        "\n",
        "Para ello, vamos a hacer uso de este dataset: https://stackabuse.s3.amazonaws.com/files/hierarchical-clustering-with-python-and-scikit-learn-shopping-data.csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr8k6SWrQDOh"
      },
      "source": [
        "!rm hier*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uARsYCM7Ppc7"
      },
      "source": [
        "!wget https://stackabuse.s3.amazonaws.com/files/hierarchical-clustering-with-python-and-scikit-learn-shopping-data.csv\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Zyu_MKQFjO"
      },
      "source": [
        "import pandas as pd\n",
        "customer_data = pd.read_csv('hierarchical-clustering-with-python-and-scikit-learn-shopping-data.csv')\n",
        "customer_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQNNm4L6QbDC"
      },
      "source": [
        "Para poder visualizarlo, vamos a quedarnos con las 2 columnas, y vamos a realizar un clustering con ellas para ver qué tipos diferentes de clientes tenemos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldYHiOt5QS63"
      },
      "source": [
        "data = customer_data.iloc[:, 3:].values\n",
        "print(data[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqWi84QXQ4WA"
      },
      "source": [
        "from matplotlib._api import define_aliases\n",
        "# realizamos el clustering\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Dendograma de nuestros clientes\")\n",
        "## Aquí tu código ##\n",
        "## Aquí tu código ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKPqEQ0hRV9H"
      },
      "source": [
        "Fijaos que parece que encuentra 5 tipos de clientes más o menos diferenciados (cortando en 150, donde existe la mayor distancia vertical para el cluster de más a la derecha). Pero lo único que tenemos en Z es la clasificación de los clientes existentes. ¿Y si quisieramos predecir un nuevo cliente?\n",
        "\n",
        "Necesitamos hacer uso de `AgglomerativeClustering`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h89ZXdZRgS1"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster=## Aquí tu código ##\n",
        "clusters=## Aquí tu código ##\n",
        "print(clusters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w22QCV-kRxlv"
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}