{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iE9XTGLU2V"
      },
      "source": [
        "# T4 - Métodos de análisis de componentes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbWgfBEtNbgS"
      },
      "source": [
        "En esta clase vamos a ver cómo funciona el Análisis de Componentes Principales (PCA, de aquí en adelante).\n",
        "\n",
        "### Implementando PCA desde 0\n",
        "\n",
        "Para ello, vamos a implementar nosotros PCA de forma manual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnz1RovGRbHc"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import mean\n",
        "from numpy import cov\n",
        "from numpy.linalg import eig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# definmos nuestros datos\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)\n",
        "plt.scatter(A[:, 0], A[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4wiUZIR6-x"
      },
      "source": [
        "# calculamos la media de cada columna (variable)\n",
        "M = mean(A, axis=0)\n",
        "print(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjj4x4LMR8WT"
      },
      "source": [
        "# centramos los datos restandoles la media (por variable)\n",
        "# restarla a lso datos la media para estandarizar o normalizar , eneste caso solo se resta la media\n",
        "C = A - M\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iRwKpgZR_Pj"
      },
      "source": [
        "# calculamos la covarianza de los datos centrados\n",
        "# recordar que si se tiene dos vriables va ser 2x2\n",
        "V = cov(C.T)\n",
        "print(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMqqCjZdSAkT"
      },
      "source": [
        "# eigendecomposition de la matriz de covarianza\n",
        "# esta funcion me saca lso valores propios y los vectores propios\n",
        "values, vectors = ## Tu código aqui ##\n",
        "print(vectors)\n",
        "print(values)\n",
        "#los vectores propios se ordenan de mayor a menor en funcion del valor propios\n",
        "# en pocas palabras los primeros son losmas importasntes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLH0HGtzSDPN"
      },
      "source": [
        "# proyectamos datos\n",
        "#muliplico los datos por los vectores propios\n",
        "P = C.dot(vectors)\n",
        "print(P)\n",
        "plt.scatter(P[:, 0], P[:, 1])\n",
        "# recordar qu eno se redujo al dimensionaldiad\n",
        "# el resultado aqui son los mismos datos\n",
        "# puestos anteriormente solo que esta vez los tengo en otra proyeccion pero es\n",
        "# la misma informacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1hYB6a2fgAo"
      },
      "source": [
        "Ahora vamos a hacer lo mismo con las librerías de `scikit-learn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtnntlnUfdMp"
      },
      "source": [
        "# SKLEARN TIENE UNA FUNCION QUE YA HACE TODO POR DENTRO LO MIMSO DE LOS VECTORES\n",
        "# PORPIOS Y DEMAS ENTONCES LA FUNCION QUEDAA\n",
        "from numpy import array\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# instanciamos el algoritmo PCA\n",
        "pca = ## Tu código aqui ##\n",
        "\n",
        "# lo ajustamos a los datos\n",
        "pca.fit(A)\n",
        "\n",
        "# mostramos las componentes principales calculadas y la varianza explicada\n",
        "print(pca.components_) # saca cada comp'onentes\n",
        "print(pca.explained_variance_)\n",
        "\n",
        "# transformamos los datos usando estas componentes\n",
        "B = ## Tu código aqui ##\n",
        "\n",
        "print(B)\n",
        "\n",
        "plt.scatter(B[:, 0], B[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-49hssRz2g"
      },
      "source": [
        "Tras haber visto lo sencillo que es el PCA, vamos a hacer un primer ejemplo muy sencillo.\n",
        "\n",
        "### Ejemplo sencillo con `scikit-learn`\n",
        "\n",
        "Empezamos importando las librerías necesarias y creándonos un dataset de juguete:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIGbiELUNoWY"
      },
      "source": [
        "# importamos las librerías necesarias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# nos creamos nuestro dataset aleatorio\n",
        "X = np.zeros((50, 2))\n",
        "X[:, 0] = np.linspace(0, 50)\n",
        "X[:, 1] = X[:, 0]\n",
        "X = X + 5*np.random.randn(X.shape[0], X.shape[1])\n",
        "\n",
        "# lo mostramos\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(X[:,0], X[:, 1], alpha=0.5, c='green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlTbv9QEO6Wy"
      },
      "source": [
        "Fijaos que los datos no están centrados ni tienen varianza 1. Hay que estandarizarlos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHy284w7PALv"
      },
      "source": [
        "# estandarizamos los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "Xs = StandardScaler().fit_transform(X)\n",
        "\n",
        "# los mostramos\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(Xs[:,0], Xs[:, 1], alpha=0.5, c='green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMSfiYakPOuA"
      },
      "source": [
        "Ahora podemos aplicar la PCA a nuestros datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtXWV7vZODTS"
      },
      "source": [
        "# aplicamos la PCA para reducir las variables de 2 a 1\n",
        "pca_reduce = ## Tu código aqui ##\n",
        "# proyectamos los datos en la nueva componente\n",
        "X_proj = ## Tu código aqui ##\n",
        "# los re-proyectamos en el espacio original usando solo 1 componente\n",
        "X_rebuild =## Tu código aqui ##\n",
        "# esto lo vamos a necesitar para visualizar los ejes que tendríamos si n_componentes=2\n",
        "pca = PCA(n_components=2)\n",
        "_ = pca.fit_transform(Xs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WVcb0cCOfiE"
      },
      "source": [
        "Visualizamos el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_8J65StOhWu"
      },
      "source": [
        "# para visualizarlo bonito\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(Xs[:,0], Xs[:, 1], alpha=0.5, c='green')\n",
        "plt.scatter(X_rebuild[:, 0], X_rebuild[:, 1], alpha=0.3, c='r')\n",
        "\n",
        "# plot the components\n",
        "soa = np.hstack((\n",
        "    np.ones(pca.components_.shape) * pca.mean_,\n",
        "    pca.components_ * np.atleast_2d(\n",
        "        # components scaled to the length of their variance\n",
        "        np.sqrt(pca.explained_variance_)\n",
        "    ).transpose()\n",
        "))\n",
        "x, y, u, v = zip(*soa)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.quiver(\n",
        "    x, y, u, v,\n",
        "    angles='xy',\n",
        "    scale_units='xy',\n",
        "    scale=0.5,\n",
        "    color='r'\n",
        ")\n",
        "plt.axis('scaled')\n",
        "plt.draw()\n",
        "\n",
        "plt.legend([\n",
        "    'original',\n",
        "    'projection'\n",
        "])\n",
        "\n",
        "# plot the projection errors\n",
        "for p_orig, p_proj in zip(Xs, X_rebuild):\n",
        "    plt.plot([p_orig[0], p_proj[0]], [p_orig[1], p_proj[1]], c='g', alpha=0.3)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnd8kk_OM1Rd"
      },
      "source": [
        "Aunque a primera vista PCA pueda parecer similar a la Regresión Lineal, no son lo mismo. Vamos a verlo con este ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxHNn0h7MwjW"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(Xs[:,0].reshape(-1, 1), Xs[:, 1])\n",
        "# coef_ son los coeficientes de regresión\n",
        "y_pred = Xs[:,0] * lin_reg.coef_\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "# mostramos los datos\n",
        "plt.scatter(Xs[:,0], Xs[:, 1], alpha=0.5, c='green')\n",
        "plt.scatter(X_rebuild[:, 0], X_rebuild[:, 1], alpha=0.3, c='r')\n",
        "plt.scatter(Xs[:,0], y_pred, alpha=0.5, c='blue')\n",
        "\n",
        "# mostramos los errores de proyection (PCA)\n",
        "for p_orig, p_proj in zip(Xs, X_rebuild):\n",
        "    plt.plot([p_orig[0], p_proj[0]], [p_orig[1], p_proj[1]], c='r', alpha=0.3)\n",
        "\n",
        "# mostramos los errores de prediccion (LinearRegression)\n",
        "for p_orig, y in zip(Xs, np.hstack((Xs[:,0].reshape(-1, 1), y_pred.reshape(-1, 1)))):\n",
        "    plt.plot([p_orig[0], y[0]], [p_orig[1], y[1]], c='b', alpha=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OITrganRNAHD"
      },
      "source": [
        "Aquí podéis observar las diferencias entre ellas:\n",
        "\n",
        "- PCA trata de minimizar el error de proyección de las muestras a nuestras nuevas \"bases\" en las que estamos representando los datos tras la proyección (lineas rojas).\n",
        "- Regresión lineal trata de minimizar el error de predicción (lineas azules)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV8OBwCGgkyv"
      },
      "source": [
        "### Eigenfaces y PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiH1RUjUCl7Y"
      },
      "source": [
        "En este ejemplo vamos a ver un caso de uso muy común de la PCA: la descomposición de caras en \"caras base\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o82qAMmNgkZH"
      },
      "source": [
        "# Before using PCA, let us try and understand as well as display the original images\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "oliv=fetch_olivetti_faces()\n",
        "print(oliv.keys())\n",
        "print (oliv.data.shape) #tells us there are 400 images that are 64 x 64 (4096) pixels each - See more at: https://shankarmsy.github.io/posts/pca-sklearn.html#sthash.h4DDqQzH.dpuf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUvrDnMWhdB0"
      },
      "source": [
        "# visualizamos las caras\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# cada cara es de 64x64 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(oliv.images[i], cmap=plt.cm.bone, interpolation='nearest')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRdIH7mhqLr"
      },
      "source": [
        "# veamos cuanta de la varianza explicada tendríamos al comprimir las imágenes a 8x8 (64 pixels)\n",
        "X,y=oliv.data, oliv.target\n",
        "pca_oliv = PCA(64)\n",
        "X_proj = pca_oliv.fit_transform(X)\n",
        "print(X_proj.shape)\n",
        "cumvar = np.cumsum(pca_oliv.explained_variance_ratio_)# ir sumando las varianzas de las componentes\n",
        "print(cumvar)\n",
        "plt.plot(cumvar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YRFBXLgh7-0"
      },
      "source": [
        "Fijaos que hemos conseguido retener el 89.7% de la varianza explicada después de haber comprimido las 4096 dimensiones del dataset original a 64, haciendo uso de las primeras 64 componentes principales.\n",
        "\n",
        "Cada una de estas componentes principales explica cierta cantidad de varianza del dataset original. El parámetro components_ del estimador nos da las componentes con máxima varianza explicada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlHkE3w0D5XK"
      },
      "source": [
        "print(pca_oliv.components_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5vrqn-hEE9w"
      },
      "source": [
        "Vamos a visualizar nuestro dataset reducido a 64 componentes. Fijaos que lo que estamos viendo NO es una reconstrucción del dataset original, sino las componentes principales o \"bases\" de nuestros datos reducidos. Las componentes principales son vectores de longitud igual al número de características original (4096), así que lo reshapeamos a 64x64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpWWpujxh7fU"
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# mostramos las caras base\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.reshape(pca_oliv.components_[i,:], (64,64)), cmap=plt.cm.bone, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EChLdfpwiNfA"
      },
      "source": [
        "#Awesome, let's now try to reconstruct the images using the new reduced dataset. In other words, we transformed the\n",
        "#64x64 pixel images into 8x8 images. Now to visualize how these images look we need to inverse transform the 8x8 images\n",
        "#back to 64x64 dimension. Note that we're not reverting back to the original data, we're simply going back to the\n",
        "#actual dimension of the original images so we can visualize them.\n",
        "\n",
        "X_inv_proj = pca_oliv.inverse_transform(X_proj)\n",
        "# reshapeamos como 400 imagenes de 64x64 dimensiones\n",
        "X_proj_img = np.reshape(X_inv_proj,(400,64,64))\n",
        "\n",
        "# visualizamos las caras originales\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(oliv.images[i], cmap=plt.cm.bone, interpolation='nearest')\n",
        "\n",
        "# mostramos las caras reconstruidas\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(X_proj_img[i], cmap=plt.cm.bone, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY9XxIo3ETgC"
      },
      "source": [
        "Nada mal, no os parece?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGuagMvMHOQ"
      },
      "source": [
        "FUENTES:\n",
        "- https://machinelearningmedium.com/2018/04/22/principal-component-analysis/\n",
        "- https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n",
        "- https://shankarmsy.github.io/posts/pca-sklearn.html\n",
        "\n",
        "RECURSOS:\n",
        "\n",
        "- https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
        "- https://plot.ly/python/v3/ipython-notebooks/principal-component-analysis/\n",
        "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
        "- https://medium.com/@kyasar.mail/pca-principal-component-analysis-729068e28ec8\n",
        "- http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html (básico)\n",
        "- https://sebastianraschka.com/Articles/2014_pca_step_by_step.html (avanzado)"
      ]
    }
  ]
}