{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iE9XTGLU2V"
      },
      "source": [
        "# T2.5 - Métodos de clustering probabilísticos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpRUPzfm-ZM8"
      },
      "source": [
        "En esta práctica vamos a ver cómo funciona el clustering probabilístico, en concreto, el basado en Mixtura de Modelos Gaussianos, que hace uso de modelos generativos para encontrar el mejor generador (compuesto por varias componentes) para los datos de los que disponemos y, de esa manera, ser capaces de separarlos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlDCQMN49Ga7"
      },
      "source": [
        "## Gaussian Mixture Models\n",
        "\n",
        "Vamos a recordar cómo funciona primero el k-means, en qué se parece al GMM y por qué podemos considerar el GMM como una generalización más potente del k-means.\n",
        "\n",
        "Empezamos, como siempre, con los imports necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jexyV7qw9Ga8"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Js6fKJmj9Ga_"
      },
      "source": [
        "### Las debilidades del k-Means\n",
        "\n",
        "Está claro que el k-means es un algoritmo muy utilizado dentro del clustering, pero tiene sus debilidades.\n",
        "\n",
        "Vamos a entenderlas mediante dos ejemplos.\n",
        "\n",
        "Antes de nada, vamos a generar los datos para ambos ejemplos y a visualizarlos. Simplemente vamos a usar un dataset artificialmente generado con 3 centros, y luego vamos a \"estirar\" dicho dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1774wuZG9GbA"
      },
      "source": [
        "# generamos nuestro dataset original\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=400, centers=3,\n",
        "                       cluster_std=0.75, random_state=0)\n",
        "X = X[:, ::-1] # flip axes for better plotting\n",
        "# mostramos los datos originales\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, s=40, cmap='viridis');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u0go11mHBMX"
      },
      "source": [
        "# estiramos los datos\n",
        "rng = np.random.RandomState(13)\n",
        "X_stretched = np.dot(X, rng.randn(2, 2))\n",
        "# mostramos los datos\n",
        "plt.scatter(X_stretched[:, 0], X_stretched[:, 1], c=y_true, s=40, cmap='viridis');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEMCJ0_DHTeF"
      },
      "source": [
        "Ahora vamos a aplicar el mismo K-means exactamente sobre cada uno de ellos y observar los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-dUrs6wO9GbI"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None, title=''):\n",
        "    labels = kmeans.fit_predict(X)\n",
        "\n",
        "    # plot the input data\n",
        "    ax = ax or plt.gca()\n",
        "    ax.axis('equal')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
        "\n",
        "    # plot the representation of the KMeans model\n",
        "    centers = kmeans.cluster_centers_\n",
        "    radii = [cdist(X[labels == i], [center]).max()\n",
        "             for i, center in enumerate(centers)]\n",
        "    for c, r in zip(centers, radii):\n",
        "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\n",
        "\n",
        "    ax.scatter(centers[:, 0], centers[:, 1], marker='*', c='w', s=300, zorder=3)\n",
        "    ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "pgU8Buxb9GbK"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [16, 8]\n",
        "fig, ax = plt.subplots(1,2)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "plot_kmeans(kmeans, X, ax=ax[0], title='Dataset original')\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "plot_kmeans(kmeans, X_stretched, ax=ax[1], title='Dataset estirado')\n",
        "plt.rcParams['figure.figsize'] = [8, 8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRVrKqvIvEJ"
      },
      "source": [
        "Si os fijáis, podemos observar varios problemas a simple vista:\n",
        "\n",
        "- El k-means utilizar círculos (o en dimensions superiores, esferas o hiper-esferas) para delimitar los clusters, lo que muchas veces no se ajusta a la distribución real de nuestros datos. Esto es debido a la asunción de **independencia** entre las variables que usamos para clusterizar (covarianza es 0). Fijaos que realmente el tamaño del cluster no es el círculo blanco, sino el punto medio entre centroides.\n",
        "\n",
        "- El k-means no permite decidir el tamaño de estos círculos (hiper-esferas), este tamaño viene dado por la posición de los centroides de cada cluster. Con lo cual, en caso de tener clusters de tamaño dispar, el kmeans no va a poder clasificarlos correctamente.\n",
        "\n",
        "- El k-means realiza asignaciones deterministas (hard-classification) de las muestras a los clusters, incluso cuando no está claro si pertenecen a uno u otro. Parece útil que si un elemento está entre dos clusters, en vez de asignarlo a uno u a otro, le asignemos un 50% de probabilidad de pertenecer a cada uno, no os ¿parece?\n",
        "\n",
        "Fijaos que todas estas limitaciones parecen bastante restrictivas, ¿verdad? ¿Y si encontraramos una generalización del k-means que solucionara estos problemas?\n",
        "\n",
        "Pues si, los modelos de mezcla de gaussianas solucionan estos problemas!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "P814kPws9GbS"
      },
      "source": [
        "## Generalizando el algoritmo de k-means (Expectation–Maximitation): Gaussian Mixture Models\n",
        "\n",
        "Como ya sabemos, un modelo de mezcla de Gaussianas trata de encontrar una mezcla de distribuciones de probabilidad Gaussianas multidimensionales que sean capaces de **generar** los datos con los que tratamos.\n",
        "\n",
        "En el caso más sencillo, los GMMs pueden encontrar los clusters de la misma forma que k-means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "mfUdfAFy9GbS"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "## Aquí tu código ##\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3OIboB0b9GbV"
      },
      "source": [
        "Pero fijaos que como GMM es un modelo probabilístico, no solo podemos encontrar a qué cluster pertenece cada muestra, sino también con qué probabilidad.\n",
        "\n",
        "Esto lo podemos hacer usando la función ``predict_proba``, que devuelve una matriz de tamaño ``[n_samples, n_clusters]`` que mide la probabilidad de cada punto de pertenecer a cada cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BsAuB5Rb9GbW"
      },
      "source": [
        "probs = ## Aquí tu código ##\n",
        "print(probs[40:50].round(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AhOqkEE89GbZ"
      },
      "source": [
        "Fijaos que esta información es realmente útil. Si le asignamos un tamaño diferente a cada punto dependiendo de la certeza de su predicción, podemos visualizar gráficos como el siguiente, donde los puntos más pequeños son los que se encuentran entre dos clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bW50BRcC9GbZ"
      },
      "source": [
        "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size, edgecolor='k');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DGyuneVo9Gbc"
      },
      "source": [
        "Útil, no os parece?\n",
        "\n",
        "Recordad que al final, un GMM es tremendamente similar to a un k-means, usa el algoritmo de esperanza-maximización:\n",
        "\n",
        "1. Escoge unas estimaciones iniciales para la posición ($\\mu_k$) y la forma ($\\sum_k$) de cada cluster (**componente**)\n",
        "\n",
        "2. Después, repite hasta converger los siguientes pasos:\n",
        "\n",
        "   1. *Paso E*: para cada elemento de nuestro dataset, encuentra unos pesos ($\\hat{z}_{ik}$) codifican la pertenencia de elemento punto a cada componente de nuestro GMM\n",
        "   2. *Paso M*: para cada cluster (**componente**), actualiza su posición, forma y tamaño, utilizando los pesos anteriores y todas las muestras del dataset para calcular $\\hat{\\pi}_k$, $\\hat{\\mu}_k$ y $\\hat{\\Sigma}_k$\n",
        "\n",
        "Y al final, cada elemento está asociado con las probabilidades de pertenecer a cada una de las componentes de nuestro GMM (**clusteres**).\n",
        "\n",
        "Igual que en la versión del k-means, este algoritmo es sensible a la inicialización y puede no encontrar el máximo global, quedándose atrapado en un máximo local. Por ello, en la práctica utilizamos varias ejecuciones con distintas inicializaciones.\n",
        "\n",
        "La siguiente función permite visualizar las elipses mostrando las curvas de nivel de las gaussianas en 2D:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MBJ41ZPU9Gbd"
      },
      "source": [
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
        "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Convert covariance to principal axes\n",
        "    if covariance.shape == (2, 2):\n",
        "        U, s, Vt = np.linalg.svd(covariance)\n",
        "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
        "        width, height = 2 * np.sqrt(s)\n",
        "    else:\n",
        "        angle = 0\n",
        "        width, height = 2 * np.sqrt(covariance)\n",
        "\n",
        "    # Draw the Ellipse\n",
        "    for nsig in range(1, 4):\n",
        "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
        "                             angle=angle, **kwargs))\n",
        "\n",
        "def plot_gmm(gmm, X, label=True, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    labels = gmm.fit(X).predict(X)\n",
        "    if label:\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
        "    else:\n",
        "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
        "    ax.axis('equal')\n",
        "\n",
        "    w_factor = 0.2 / gmm.weights_.max()\n",
        "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
        "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
        "\n",
        "    ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1], marker='*', c='w', s=300, zorder=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qh7zy_g_9Gbh"
      },
      "source": [
        "Vamos a usarla para visualizar el clustering anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wOkQEYxB9Gbi"
      },
      "source": [
        "gmm = ## Aquí tu código ##\n",
        "plot_gmm(gmm, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hECJaexP9Gbz"
      },
      "source": [
        "Hasta ahora solo hemos visto cómo se comporta con el dataset original, vamos a ver qué tal con el \"estirado\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYaycBtedrI7"
      },
      "source": [
        "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)\n",
        "plot_gmm(gmm, X_stretched)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "enCH-zIy9Gb2"
      },
      "source": [
        "Fijaos que como hemos comentado, el GMM es capaz de dar solución a las limitaciones del k-means comentadas anteriormente.\n",
        "\n",
        "Fijaos también que le hemos indicado un parámetro `covariance_type`. Este parámetro simplemente indica cómo va a ser la matriz de covarianzas. Puede tomar los siguientes valores:\n",
        "\n",
        "- `spherical`: únicamente la diagonal principal de la matriz de covarianzas va a ser distinta de cero, y además todos los valores van a ser iguales\n",
        "- `diag`: únicamente la diagonal principal de la matriz de covarianzas va a ser distinta de cero.\n",
        "- `full`: sin restricciones en la matriz de covarianzas.\n",
        "\n",
        "Con lo que sabéis, ¿qué creéis que va a implicar cada una de estas opciones?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2j1tU-rS9Gb4"
      },
      "source": [
        "## GMM como estimador de densidad\n",
        "\n",
        "Aunque hayamos categorizado al GMM como un algoritmo de clustering, estamos hablando, fundamentalmente, de un algoritmo de estimación de densidad.\n",
        "\n",
        "Es decir, estamos hablando de un modelo probabilístico generativo capaz de describir la distribución de los datos (y cuanto mejor la describa, mejor seremos capaces de agruparlos).\n",
        "\n",
        "Por ejemplo, si cogemos como dataset el siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZT3nJ1wv9Gb5"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n",
        "plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "M1WQaepo9Gb8"
      },
      "source": [
        "Podemos intentar encontrar un GMM capaz de **generar** los datos. Vamos a probar con 2 componentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8q08lV1L9Gb9"
      },
      "source": [
        "gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\n",
        "plot_gmm(gmm2, Xmoon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7QhoSgLQ9Gb_"
      },
      "source": [
        "Claramente, no es suficiente. Vamos a ver con 10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "eV7UL0US9GcA"
      },
      "source": [
        "gmm10 = GaussianMixture(n_components=10, covariance_type='full', random_state=0)\n",
        "plot_gmm(gmm10, Xmoon, label=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cSc1PBTz9GcE"
      },
      "source": [
        "Si os fijáis, este GMM es capaz de **generar** perfectamente los datos que tenemos. Lo que está ocurriendo en este caso no es que estemos clusterizando los datos, sino que estamos modelando la *distribución* de los datos, y si quisieramos, podríamos generar más:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "F_Dqo09X9GcF"
      },
      "source": [
        "Xnew, y = gmm10.sample(400)\n",
        "plt.scatter(Xnew[:, 0], Xnew[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xjuf9Xny9GcG"
      },
      "source": [
        "Como podéis ver, GMM es capaz de modelar practicamente cualquier distribución de datos multi-dimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "h5wMVIP-9GcI"
      },
      "source": [
        "### Cómo escogemos cuantas componentes?\n",
        "\n",
        "The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset.\n",
        "A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the *likelihood* of the data under the model, using cross-validation to avoid over-fitting.\n",
        "Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) or the [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
        "Scikit-Learn's ``GMM`` estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach.\n",
        "\n",
        "Let's look at the AIC and BIC as a function as the number of GMM components for our moon dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yXEoCLBQ9GcI"
      },
      "source": [
        "n_components = np.arange(1, 21)\n",
        "models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(Xmoon) for n in n_components]\n",
        "\n",
        "plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')\n",
        "plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('n_components');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "y_l5RedW9GcK"
      },
      "source": [
        "The optimal number of clusters is the value that minimizes the AIC or BIC, depending on which approximation we wish to use. The AIC tells us that our choice of 16 components above was probably too many: around 8-12 components would have been a better choice.\n",
        "As is typical with this sort of problem, the BIC recommends a simpler model.\n",
        "\n",
        "Notice the important point: this choice of number of components measures how well GMM works *as a density estimator*, not how well it works *as a clustering algorithm*.\n",
        "I'd encourage you to think of GMM primarily as a density estimator, and use it for clustering only when warranted within simple datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "aCHgh3jn9GcL"
      },
      "source": [
        "## Ejemplo: Vamos a generar nuevos datos\n",
        "\n",
        "Vamos a tratar de generar nuevos dígitos similares a los disponibles en el dataset `digits`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Lw-0HWqh9GcL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "oRS64xH69GcN"
      },
      "source": [
        "Veamos algunas imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5ICKUVpX9GcO"
      },
      "source": [
        "def plot_digits(data):\n",
        "    fig, ax = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
        "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "    for i, axi in enumerate(ax.flat):\n",
        "        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\n",
        "        im.set_clim(0, 16)\n",
        "plot_digits(digits.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "pPJ_b8Hh9GcP"
      },
      "source": [
        "Tenemos 1800 dígitos de 8x8 píxels, es decir, 64 dimensiones. Vamos a ver cuantas componentes necesitamos para *modelar* nuestros datos 64-dimensionales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "x9vRCtzb9GcT"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "n_components = np.arange(50, 210, 10)\n",
        "models = [GaussianMixture(n, covariance_type='full', random_state=0) for n in n_components]\n",
        "aics = [model.fit(digits.data).aic(digits.data) for model in models]\n",
        "plt.plot(n_components, aics);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2XeQa6AL9GcV"
      },
      "source": [
        "De acuerdo al AIC, parece que `n_componentes=110` es una buena estimación. Vamos a crear nuestro modelo GMM y ver si ha convergido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6q_NO8QX9GcV"
      },
      "source": [
        "gmm = GaussianMixture(n_components=110, covariance_type='full', random_state=0)\n",
        "gmm.fit(digits.data)\n",
        "print(gmm.converged_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KCfBsv3y9GcY"
      },
      "source": [
        "Perfecto. Ahora podemos simplemente muestrear nuevos elementos de nuestra mixtura de gaussianas, y deberíamos obtener imágenes parecidas. Veamos si es verdad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "OOiN1JYd9GcZ"
      },
      "source": [
        "data_new, y_new = gmm.sample(100)\n",
        "data_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rcw1A4JM9Gcb"
      },
      "source": [
        "Vamos a visualizar nuestros nuevos datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hQ8YEan39Gcb"
      },
      "source": [
        "plot_digits(data_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "teZenMn69Gcd"
      },
      "source": [
        "¡Fijaos que somos capaces efectivamente de generar nuevos datos como los que teníamos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88fTu1iKZKWb"
      },
      "source": [
        "FUENTE: https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
      ]
    }
  ]
}